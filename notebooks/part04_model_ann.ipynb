{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8811b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nn = torch.from_numpy(X_train.to_numpy()).float()\n",
    "y_train_nn = torch.squeeze(torch.from_numpy(y_train.to_numpy()).float())\n",
    "X_val_nn = torch.from_numpy(X_val.to_numpy()).float()\n",
    "y_val_nn = torch.squeeze(torch.from_numpy(y_val.to_numpy()).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a292c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device\n",
    "#device(type='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0500d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432cc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 1\n",
    "input_size = X.shape[1]\n",
    "num_classes = 1\n",
    "learning_rate = 0.01\n",
    "hidden_size_1 = 200\n",
    "hidden_size_2 = 400\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, dropout_rate):\n",
    "        super(Net, self).__init__()\n",
    "        self.hiden_layer1 = nn.Linear(input_size, hidden_size_1)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.hiden_layer2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.output = nn.Linear(hidden_size_2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hiden_layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.hiden_layer2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48586f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size=input_size, hidden_size_1=hidden_size_1, hidden_size_2=hidden_size_2, dropout_rate=dropout_rate)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b080373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train_nn, y_train_nn = X_train_nn.to(device), y_train_nn.to(device)\n",
    "X_val_nn, y_val_nn = X_val_nn.to(device), y_val_nn.to(device)\n",
    "\n",
    "net = net.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    predicted = y_pred.ge(.5).view(-1)\n",
    "    acc =  (y_true == predicted).sum().float() / len(y_true)\n",
    "    \n",
    "def calculate_f1(y_true, y_pred):\n",
    "    #Class Probability\n",
    "    predicted = y_pred.ge(.5).squeeze().long()\n",
    "    y_true = y_true.long()\n",
    "    #F1 Score Calculator\n",
    "    f1 = f1_score(y_true, predicted, average='micro')\n",
    "\n",
    "    return f1\n",
    "\n",
    "def calculate_precision(y_true, y_pred):\n",
    "    #Class Probability\n",
    "    predicted = y_pred.ge(.5).squeeze().long()\n",
    "    y_true = y_true.long()\n",
    "    #F1 Score Calculator\n",
    "    precision = precision_score(y_true, predicted)\n",
    "\n",
    "    return precision \n",
    "\n",
    "def round_tensor(t, decimal_places=3):\n",
    "    return round(t.item(), decimal_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f25758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-Paramters\n",
    "early_stop_epochs = 5\n",
    "patience = 0\n",
    "best_val_loss = float('inf')\n",
    "num_epoch = 500\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_pred = net(X_train_nn)\n",
    "    y_pred = torch.squeeze(y_pred)\n",
    "    train_loss = criterion(y_pred, y_train_nn)\n",
    "    if epoch % 10 == 0:\n",
    "        train_score = calculate_f1(y_train_nn, y_pred)\n",
    "        y_val_pred = net(X_val_nn)\n",
    "        y_val_pred = torch.squeeze(y_val_pred)\n",
    "        val_loss = criterion(y_val_pred, y_val_nn)\n",
    "        val_score = calculate_f1(y_val_nn, y_val_pred)        \n",
    "        print(f'epoch {epoch} - train loss: {round_tensor(train_loss)}, train f1 : {round_tensor(train_score)} val loss: {round_tensor(val_loss)}, val f1: {round_tensor(val_score)}')\n",
    "        # Verify boost on validation set\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_stop_epochs:\n",
    "                print('Early Stopping')\n",
    "                break\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    predictions.append({'epoch': epoch, 'y_pred': y_pred.detach().numpy(), 'y_val_pred': y_val_pred.detach().numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f556e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nn = torch.from_numpy(X.to_numpy()).float()\n",
    "X_train_nn = torch.from_numpy(X_train.to_numpy()).float()\n",
    "previsão = net(X_nn).detach().numpy()\n",
    "previsão = previsão.round(5)\n",
    "results['nn_prob'] = previsão\n",
    "results['nn_pred'] = results['nn_prob'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "CM(data.is_canceled, results.nn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e57c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_ann(trial):\n",
    "    input_size = X.shape[1]\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True)\n",
    "    hidden_size_1 = trial.suggest_int('hidden_size_1', 100, 400, step=50)\n",
    "    hidden_size_2 = trial.suggest_int('hidden_size_2', 100, 400, step=50)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.4, step=0.1)\n",
    "\n",
    "    net = Net(input_size=input_size, hidden_size_1=hidden_size_1, hidden_size_2=hidden_size_2, dropout_rate=dropout_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    early_stop_epochs = 30\n",
    "    patience = 0\n",
    "    best_val_loss = float('inf')\n",
    "    num_epoch = 300\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        y_pred = net(X_train)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        train_loss = criterion(y_pred, y_train)\n",
    "        if epoch % 10 == 0:\n",
    "            train_score = calculate_f1(y_train, y_pred)\n",
    "            y_val_pred = net(X_val)\n",
    "            y_val_pred = torch.squeeze(y_val_pred)\n",
    "            val_loss = criterion(y_val_pred, y_val)\n",
    "            val_acc = calculate_f1(y_val, y_val_pred)\n",
    "            # Verificar se houve melhora no conjunto de validação\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= early_stop_epochs:\n",
    "                    break\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return val_acc\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_ann, n_trials=100)\n",
    "\n",
    "trial = study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the result\n",
    "nn_best_params = study.best_params\n",
    "nn_best_score = study.best_value\n",
    "print(f\"Best score: {nn_best_score}\\n\")\n",
    "print(f\"Optimized parameters: {nn_best_params}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a47c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dir = os.path.join(ROOT_DIR, \"config\", \"params\")\n",
    "os.makedirs(params_dir, exist_ok=True) #O parâmetro exist_ok=True faz com que a função não retorne um erro caso a pasta já exista.\n",
    "now = dt.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "filename_xgb = f'xgb_best_params_{now}_v1.0.json'\n",
    "params_path = os.path.join(params_dir, filename_xgb)\n",
    "\n",
    "\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(xgb_best_params, f)\n",
    "\n",
    "    \n",
    "# for dirname, _, filenames in os.walk(f'{ROOT_DIR}\\config\\params'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['is_canceled','lgbm_prob','lgbm_pred','xgb_prob','xgb_pred','nn_prob','nn_pred']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcedcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import logging\n",
    "\n",
    "# logger = logging.getLogger()\n",
    "\n",
    "# logger.setLevel(logging.INFO)  # Setup the root logger.\n",
    "# logger.addHandler(logging.FileHandler(\"foo.log\", mode=\"w\"))\n",
    "\n",
    "# optuna.logging.enable_propagation()  # Propagate logs to the root logger.\n",
    "# optuna.logging.disable_default_handler()  # Stop showing logs in sys.stderr.\n",
    "\n",
    "# study = optuna.create_study()\n",
    "\n",
    "# logger.info(\"Start optimization.\")\n",
    "# study.optimize(objective, n_trials=10)\n",
    "\n",
    "# with open(\"foo.log\") as f:\n",
    "#     assert f.readline().startswith(\"A new study created\")\n",
    "#     assert f.readline() == \"Start optimization.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbfefef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.concat([X,y], axis=1)\n",
    "# train.loc[train.is_canceled ==1].tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
